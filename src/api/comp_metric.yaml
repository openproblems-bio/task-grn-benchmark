functionality:
  namespace: "metrics"
  info:
    type: metrics
    type_info:
      label: Label
      summary: A metric to evaluate the performance of the inferred GRN
      description: |
        A metric to evaluate the performance of the inferred GRN
  arguments:
    - name: --perturbation_data
      __merge__: file_perturbation_h5ad.yaml
      required: true
      direction: input
    - name: --prediction
      __merge__: file_prediction.yaml
      required: true
      direction: input
    - name: --score
      __merge__: file_score.yaml
      required: true
      direction: output
    - name: --reg_type
      type: string
      direction: input
      default: ridge
      description: name of regretion to use
      multiple: true
    - name: --subsample
      type: integer
      direction: input
      default: -1
      description: number of samples randomly drawn from perturbation data
      
  test_resources:
    - type: python_script
      path: /src/common/component_tests/run_and_check_output.py
    - path: /resources/grn-benchmark
      dest: resources/grn-benchmark